![](pic/IMG_FA3C4A39402D-1.jpeg)

在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数𝑌=𝑓(𝑋),要么是条件分布𝑃(𝑌|𝑋)。

学习算法可分为两种，一种时尝试去直接学习得到 $p(y|x)$ (例如逻辑回归)，或者尝试去学习直接将输入映射到0或1的方法（例如感知器算法），这种算法被称为 **「判别学习算法(discriminative earning algorithm)」**；而另外一种学习算法被称为 **「生成学习算法generative learning algorithm」**，这种算法会尝试对 $p(x|y)$ 以及 $p(y)$ 建模。

但是朴素贝叶斯却是生成方法，也就是直接找出特征输出Y和特征X的联合分布𝑃(𝑋,𝑌),然后用𝑃(𝑌|𝑋)=𝑃(𝑋,𝑌)/𝑃(𝑋)得出。

朴素贝叶斯很直观，计算量也不大，在很多领域有广泛的应用，这里我们就对朴素贝叶斯算法原理做一个小结。

## 朴素贝叶斯算法原理小结
　　　　
当我们为 $p(y)$ (被称为 **「class priors」)** 和 $p(x|y)$ 建模后，我们的算法会使用 **「贝叶斯定理Bayes rule」** 来计算给定 $x$ 后 $y$ 的 **「后验概率(posterior distribution)」**：

$$p(y|x) = \frac{p(x|y)p(y)}{p(x)}$$

其中分母可以通过 $p(x)=p(x|y=1)p(y=1)+p(x|y=0)p(y=0)$ 得到（针对二分类）。

对于分类问题我们需要对每种 $y$ 的情况分别进行建模。当有一个新的 $x$ 时，计算每个 $y$ 的后验概率，并取概率最大的那个 $y$ 作为预测输出。

由于只需要比较大小，而 $p(x)$ 对于大家都一样，所以可以忽略分母，得到下式：

$$\underset{y}{\text{argmax}} p(y|x)=\underset{y}{\text{argmax}} \frac{p(x|y)p(y)}{p(x)}=\underset{y}{\text{argmax}}p(x|y)p(y)$$

贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。数据大家好理解，被频率学派攻击的是先验概率，一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。虽然难以从严密的数学逻辑里推出贝叶斯学派的逻辑，但是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类。

### 1. 朴素贝叶斯相关的统计知识

我们先看看条件独立公式，如果X和Y相互独立，则有：

$$𝑃(𝑋,𝑌)=𝑃(𝑋)𝑃(𝑌)$$

我们接着看看条件概率公式：

$$𝑃(𝑌|𝑋)=\frac{𝑃(𝑋,𝑌)}{𝑃(𝑋)}$$
$$𝑃(𝑋|𝑌)=\frac{𝑃(𝑋,𝑌)}{𝑃(𝑌)}$$

或者说:

$$𝑃(𝑌|𝑋)=\frac{𝑃(𝑋|𝑌)𝑃(𝑌)}{𝑃(𝑋)}$$

接着看看全概率公式

$P(X)=\sum_k P(X|Y=Y_k)P(Y_k)$ 其中 $\sum_k P(Y_k)=1$

从上面的公式很容易得出贝叶斯公式：

$$P(Y_k|X)=\frac{P(X|Y_k)P(Y_k)}{\sum_k P(X|Y=Y_k)P(Y_k)}$$

 ### 2. 朴素贝叶斯的模型

从统计学知识回到我们的数据分析。假如我们的分类模型样本是：

$$(x_1^{(1)},x_2^{(1)},\cdots x_n^{(1)},y_1),(x_1^{(2)},x_2^{(2)},\cdots x_n^{(2)},y_2),\cdots,(x_1^{(m)},x_2^{(m)},\cdots x_n^{(m)},y_m)$$
　　　　
即我们有m个样本，每个样本有n个特征，特征输出有K个类别，定义为$C_1,C_2,\cdots,C_K$.

从样本我们可以学习得到朴素贝叶斯的先验分布 $P(Y=C_k)(k=1,2,\cdots,K)$,接着学习到条件概率分布 $P(X_1=x_1,X_2=x_2,\cdots X_n=x_n|Y=C_k)$,然后我们就可以用贝叶斯公式得到X和Y的联合分布 $P(X,Y)$ 了。联合分布 $P(X,Y)$ 定义为：

$$\begin{aligned}
P(X,Y=C_k)&=P(Y=C_k)P(X=x|Y=C_k)\\
&=P(Y=C_k)P(X_1=x_1,X_2=x_2,\cdots X_n=x_n|Y=C_k)
\end{aligned}$$

从上面的式子可以看出 $P(Y=C_k)$ 比较容易通过最大似然法求出，得到的 $P(Y=C_k)$ 就是类别$C_k$ 在训练集里面出现的频数。但是 $P(Y=C_k)P(X_1=x_1,X_2=x_2,\cdots X_n=x_n|Y=C_k)$ 很难求出,这是一个超级复杂的有n个维度的条件分布。朴素贝叶斯模型在这里做了一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出:

$$P(Y=C_k)P(X_1=x_1,X_2=x_2,\cdots X_n=x_n|Y=C_k)=P(X_1=x_1|Y=C_k)P(X_2=x_2|Y=C_k)\cdots P(X_n=x_n|Y=C_k)$$
　　　　
从上式可以看出，这个很难的条件分布大大的简化了，但是这也可能带来预测的不准确性。你会说如果我的特征之间非常不独立怎么办？如果真是非常不独立的话，那就尽量不要使用朴素贝叶斯模型了，考虑使用其他的分类方法比较好。但是一般情况下，样本的特征之间独立这个条件的确是弱成立的，尤其是数据量非常大的时候。虽然我们牺牲了准确性，但是得到的好处是模型的条件分布的计算大大简化了，这就是贝叶斯模型的选择。

最后回到我们要解决的问题，我们的问题是给定测试集的一个新样本特征 $(x_1^{(test)},x_2^{(test)},\cdots x_n^{(test)})$，我们如何判断它属于哪个类型？

既然是贝叶斯模型，当然是后验概率最大化来判断分类了。我们只要计算出所有的K个条件概率 $P(Y=C_k|X=X^{(test)})$,然后找出最大的条件概率对应的类别，这就是朴素贝叶斯的预测了。

### 3. 朴素贝叶斯的推断过程
　　　　
上节我们已经对朴素贝叶斯的模型也预测方法做了一个大概的解释，这里我们对朴素贝叶斯的推断过程做一个完整的诠释过程。

我们预测的类别 $C_{result}$ 是使 $P(Y=C_k|X=X^{(test)})$ 最大化的类别，数学表达式为：

$$\begin{aligned}
C_{result}&=\underset{C_k}{\text{argmax}} P(Y=C_k|X=X^{(test)})\\
&=\underset{C_k}{\text{argmax}} \frac{P(X=X^{(test)}|Y=C_k)P(Y=C_k)}{P(X=X^{(test)})}
\end{aligned}$$

由于对于所有的类别计算 $P(Y=C_k|X=X^{(test)})$ 时，上式的分母是一样的，都是 $P(X=X^{(test)})$，因此，我们的预测公式可以简化为：

$$\underset{C_k}{\text{argmax}}P(X=X^{(test)}|Y=C_k)P(Y=C_k)$$
　　　

接着我们利用朴素贝叶斯的独立性假设，就可以得到通常意义上的朴素贝叶斯推断公式:

$$C_{result}=\underset{C_k}{\text{argmax}}P(Y=C_k)\prod_{j=1}^n P(X_j=X_j^{(test)}|Y=C_k)$$

### 4. 朴素贝叶斯的参数估计
　　　　
在上一节中，我们知道只要求出 $P(Y=C_k)$ 和 $P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,\cdots n)$，我们通过比较就可以得到朴素贝叶斯的推断结果。这一节我们就讨论怎么通过训练集计算这两个概率。

对于 $P(Y=C_k)$ ,比较简单，通过极大似然估计我们很容易得到 $P(Y=C_k)$ 为样本类别 $C_k$ 出现的频率，即样本类别 $C_k$ 出现的次数 $m_k$ 除以样本总数m。

对于 $P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,\cdots n)$ ,这个取决于我们的先验条件：

a) 如果我们的 $X_j$ 是离散的值，那么我们可以假设 $X_j$ 符合多项式分布，这样得到 $P(X_j=X_j^{(test)}|Y=C_k)$ 是在样本类别 $C_k$ 中，特征 $X_j^{(test)}$ 出现的频率。即：

$$P(X_j=X_j^{(test)}|Y=C_k)=\frac{m_{kj^{test}}}{m_k}$$

其中 $m_k$ 为样本类别 $C_k$ 总的特征计数，而 $m_{kj^{test}}$ 为类别为 $C_k$ 的样本中，第j维特征 $X_j^{(test)}$ 出现的计数。

某些时候，可能某些类别在样本中没有出现，这样可能导致 $P(X_j=X_j^{(test)}|Y=C_k)$ 为0，这样会影响后验的估计，为了解决这种情况，我们引入了**拉普拉斯平滑**，即此时有：

$$P(X_j=X_j^{(test)}|Y=C_k)=\frac{m_{kj^{test}}+\lambda}{m_k+O_j \lambda}$$
　　　
其中 $\lambda$ 为一个大于0的常数，常常取为1。$O_j$ 为第j个特征的取值个数。

b)如果我们我们的 $X_j$ 是非常稀疏的离散值，即各个特征出现概率很低，这时我们可以假设 $X_j$ 符合伯努利分布，即特征 $X_j$ 出现记为1，不出现记为0。即只要 $X_j$ 出现即可，我们不关注 $X_j$ 的次数。这样得到 $P(X_j=X_j^{(test)}|Y=C_k)$ 是在样本类别 $C_k$ 中，$X_j^{(test)}$ 出现的频率。此时有：

$$P(X_j=X_j^{(test)}|Y=C_k)=P(X_j=1|Y=C_k)X_j^{(test)}+(1-P(X_j=1|Y=C_k))(1-X_j^{(test)})$$
　　　　
其中，$X_j^{(test)}$ 取值为0和1。

c)如果我们我们的 $X_j$ 是连续值，我们通常取 $X_j$ 的先验概率为正态分布，即在样本类别 $C_k$ 中， $X_j$ 的值符合正态分布。这样 $P(X_j=X_j^{(test)}|Y=C_k)$ 的概率分布是：

$$P(X_j=X_j^{(test)}|Y=C_k)=$$
　　　　其中𝜇𝑘和𝜎2𝑘是正态分布的期望和方差，可以通过极大似然估计求得。𝜇𝑘为在样本类别𝐶𝑘中，所有𝑋𝑗的平均值。𝜎2𝑘为在样本类别𝐶𝑘中，所有𝑋𝑗的方差。对于一个连续的样本值，带入正态分布的公式，就可以求出概率分布了。

5.  朴素贝叶斯算法过程
　　　　我们假设训练集为m个样本n个维度，如下：

(𝑥(1)1,𝑥(1)2,...𝑥(1)𝑛,𝑦1),(𝑥(2)1,𝑥(2)2,...𝑥(2)𝑛,𝑦2),...(𝑥(𝑚)1,𝑥(𝑚)2,...𝑥(𝑚)𝑛,𝑦𝑚)
　　　　共有K个特征输出类别，分别为𝐶1,𝐶2,...,𝐶𝐾,每个特征输出类别的样本个数为𝑚1,𝑚2,...,𝑚𝐾,在第k个类别中，如果是离散特征，则特征𝑋𝑗各个类别取值为𝑚𝑘𝑗𝑙。其中l取值为1,2,...𝑆𝑗，𝑆𝑗为特征j不同的取值数。

　　　　输出为实例𝑋(𝑡𝑒𝑠𝑡)的分类。

　　　　算法流程如下：

　　　　1) 如果没有Y的先验概率，则计算Y的K个先验概率：𝑃(𝑌=𝐶𝑘)=(𝑚𝑘+𝜆)/(𝑚+𝐾𝜆)，否则𝑃(𝑌=𝐶𝑘)为输入的先验概率。

　　　　2) 分别计算第k个类别的第j维特征的第l个个取值条件概率：𝑃(𝑋𝑗=𝑥𝑗𝑙|𝑌=𝐶𝑘)
　　　　　　a)如果是离散值:

𝑃(𝑋𝑗=𝑥𝑗𝑙|𝑌=𝐶𝑘)=𝑚𝑘𝑗𝑙+𝜆𝑚𝑘+𝑆𝑗𝜆
　　　　　　𝜆可以取值为1，或者其他大于0的数字。

　　　　　　b)如果是稀疏二项离散值:
𝑃(𝑋𝑗=𝑥𝑗𝑙|𝑌=𝐶𝑘)=𝑃(𝑗|𝑌=𝐶𝑘)𝑥𝑗𝑙+(1−𝑃(𝑗|𝑌=𝐶𝑘)(1−𝑥𝑗𝑙)
　　　　　　 此时𝑙只有两种取值。

　　　　　　c)如果是连续值不需要计算各个l的取值概率，直接求正态分布的参数:

𝑃(𝑋𝑗=𝑥𝑗|𝑌=𝐶𝑘)=12𝜋𝜎2𝑘‾‾‾‾‾√𝑒𝑥𝑝(−(𝑥𝑗−𝜇𝑘)22𝜎2𝑘)
　　　　　　需要求出𝜇𝑘和𝜎2𝑘。 𝜇𝑘为在样本类别𝐶𝑘中，所有𝑋𝑗的平均值。𝜎2𝑘为在样本类别𝐶𝑘中，所有𝑋𝑗的方差。

　　　　3）对于实例𝑋(𝑡𝑒𝑠𝑡)，分别计算：

𝑃(𝑌=𝐶𝑘)∏𝑗=1𝑛𝑃(𝑋𝑗=𝑥(𝑡𝑒𝑠𝑡)𝑗|𝑌=𝐶𝑘)
　　　　4）确定实例𝑋(𝑡𝑒𝑠𝑡)的分类𝐶𝑟𝑒𝑠𝑢𝑙𝑡
𝐶𝑟𝑒𝑠𝑢𝑙𝑡=𝑎𝑟𝑔𝑚𝑎𝑥⏟𝐶𝑘𝑃(𝑌=𝐶𝑘)∏𝑗=1𝑛𝑃(𝑋𝑗=𝑋(𝑡𝑒𝑠𝑡)𝑗|𝑌=𝐶𝑘)
 

　　　　 从上面的计算可以看出，没有复杂的求导和矩阵运算，因此效率很高。

### 6. 朴素贝叶斯算法小结
　　　　
朴素贝叶斯算法的主要原理基本已经做了总结，这里对朴素贝叶斯的优缺点做一个总结。

朴素贝叶斯的主要优点有：

1）朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。

2）对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练。

3）对缺失数据不太敏感，算法也比较简单，常用于文本分类。

朴素贝叶斯的主要缺点有：　　　

1） 理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型给定输出类别的情况下,假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。

2）需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。

3）由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。

4）对输入数据的表达形式很敏感。

以上就是朴素贝叶斯算法的一个总结，希望可以帮到朋友们。