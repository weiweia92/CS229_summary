{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9826cebb-7ca5-4857-86c1-ac210281f95c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f21840df-ab32-4119-8bcc-b61b0b19c7ad",
   "metadata": {},
   "source": [
    "## 1. 决策树(Decision Trees)\n",
    "\n",
    "本章将介绍决策树，一种简单而灵活的算法。我们首先将给出决策树的非线性与基于区域的特征，然后对基于区域的损失函数进行定义与对比，最后给出这些方法的优缺点（进而引出集成方法）。\n",
    "\n",
    "### 1.1 非线性\n",
    "\n",
    "决策树是一种天生支持 **「非线性」** 的机器学习算法。正式来说，如果一个方法是线性的，则其输入 $x \\in \\mathbb{R}^n$ 会输出下面形式的假设函数：\n",
    "\n",
    "$$h(x)=\\theta^T x$$\n",
    "\n",
    "其中 $\\theta \\in \\mathbb{R}^n$，截距项内置（$x_0=1$）。\n",
    "\n",
    "不能被化简为上述形式的假设函数即称为非线性。如果一个方法输出非线性的假设函数，则该方法也为非线性。之前我们介绍了对线性方法引入核函数，通过特征映射可以得到非线性的假设函数。\n",
    "\n",
    "与核方法相比，决策树则可以直接输出非线性的假设函数。下图给出了基于时间与地点判断该地区能否滑雪的数据："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc17d8-d9fd-4cea-a29d-7817bb64dbd8",
   "metadata": {},
   "source": [
    "### 2. 集成方法(Ensembling Method)\n",
    "\n",
    "集成方法就是将多个训练模型的输出结合起来，我们将使用 **「偏差-方差」** 分析以及以决策树为例来探究集成方法的利弊。\n",
    "\n",
    "首先，让我们用一个基础概率论的例子证明集成方法的优势所在。假设我们有 $n$ 个独立同分布的随机变量 $X_i$。假定对于所有 $X_i$ 均有 $\\mathbb V \\text{ar}(X_i)=\\sigma^2$.\n",
    "\n",
    "则平均值的方差为：\n",
    "\n",
    "$$\\mathbb V \\text{ar}(\\bar{X})=\\mathbb V \\text{ar}(\\frac{1}{n}\\sum_{i}X_i)=\\frac{\\sigma^2}{n}$$\n",
    "\n",
    "如果我们抛弃了独立假设，即变量仅为同分布。相对地，$X_i$ 之间的关联系数为 $\\rho$，则平均值的方差为：\n",
    "\n",
    "$$\\begin{align*}\n",
    "        \\mathbb V \\text{ar}(\\bar{X})\n",
    "        &=\\mathbb V \\text{ar}(\\frac{1}{n}\\sum_{i}X_i)\\\\\n",
    "        & = \\frac{1}{n^2} \\sum_{i,j} \\text{Cov}(X_i,X_j)\\\\\n",
    "                      & = \\frac{n\\sigma^2}{n^2}+\\frac{n(n-1)\\rho \\sigma^2}{n^2} \\\\\n",
    "                      & = \\rho \\sigma^2 +\\frac{1-\\rho}{n}\\sigma^2\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a30aa28-dd05-47dd-8655-fc9a93d921bb",
   "metadata": {},
   "source": [
    "在第三步中，使用了皮尔逊相关系数的定义 $\\rho_{X,Y}=\\frac{\\text{Cov}(X,Y)}{\\sigma_x \\sigma_y}$ 以及 $\\text{Cov}(X,X)=\\mathbb{V}\\text{ar}(X)$。\n",
    "\n",
    "现在，如果我们将每个随机变量想象为一个给定模型的误差，则增加模型数量以及降低模型之间的相关性都可以减少集成后的模型误差的方差：\n",
    "\n",
    "* 增加模型数量减少第二项的值\n",
    "\n",
    "* 降低模型之间的相关性减少第一项的值，使得各变量回归独立同分布\n",
    "\n",
    "生成不相关的模型的方法有很多种，包括：\n",
    "\n",
    "* 使用不同的算法\n",
    "\n",
    "* 使用不同的训练集\n",
    "\n",
    "* 装袋法（Bagging）\n",
    "\n",
    "* 提升法（Boosting）\n",
    "\n",
    "实际上提升法并不会降低模型之间的相关性（此处是笔记的一个矛盾点）。虽然前两种方法很直接，但其需要较大的工作量，下面将介绍后两种方法：**「装袋法」** 与 **「提升法」**，以及他们在决策树中的特殊应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7da2c-3db5-4962-aea0-da402cf9ebaa",
   "metadata": {},
   "source": [
    "#### 2.1 装袋法\n",
    "\n",
    "Bagging 代表 Bootstrap Aggregation，是一种 **「减少方差」** 的集成方法。\n",
    "\n",
    "#### 2.1.1 自助（Bootstrap）\n",
    "\n",
    "自助法是一种来源于统计学的方法，其最初的目的是测量某些估计量（如均值）的不确定性。\n",
    "\n",
    "假定我们有一个总体 $P$，希望去计算其的某个估计量，以及一个采样自 $P$ 的训练集 $S$（$S\\sim P$）。虽然我们可以通过计算 $S$ 的估计量来近似 $P$ 的估计量，但我们无法得知其与真实值的误差。为了做到这一点我们需要采样自 $P$ 的多个独立训练集 $S_1, S_2,\\cdots$。\n",
    "\n",
    "自助法的思路是假设 $S=P$，则我们可以生成一个新的 bootstrap 集 $Z$ 采样自 $S$（$Z \\sim S,\\vert Z \\vert=\\vert S \\vert$），进而生成多个这样的样本 [公式]，然后通过这些自助集上的估计量的方差来得到对误差的估计."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5f925b-6e99-4a70-b871-3d32375e3ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
