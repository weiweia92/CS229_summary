{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a10902d-ea20-4e9b-a4a3-ab812eb4eb78",
   "metadata": {},
   "source": [
    "### Bayesian Interpretation of Regularization\n",
    "\n",
    "Estimating the model of the posterior distribution is also called *maximum a posterior estimation* (MAP).That is,\n",
    "\n",
    "$$\\theta_{\\text{MAP}}=\\underset{\\theta}{\\text{argmax}} p(\\theta|x,y)$$\n",
    "\n",
    "Compare this to the *maximum likelihood estimation* (MLE) we have seen previously:\n",
    "\n",
    "$$\\theta_{\\text{MLE}}=\\underset{\\theta}{\\text{argmax}} p(y|x,\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8495e-e323-44c1-ae11-40605462d2bd",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "We have :\n",
    "\n",
    "$$p(\\theta|x,y)=\\frac{p(x,y,\\theta)}{p(x,y)}=\\frac{p(y|x,\\theta)p(x,\\theta)}{p(x,y)}=\\frac{p(y|x,\\theta)p(\\theta|x)p(x)}{p(x,y)}$$\n",
    "\n",
    "Assume that $p(\\theta)=p(\\theta|x)$, then \n",
    "\n",
    "Proof:\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\mathrm{MAP}} & = \\arg \\max_\\theta p(\\theta \\ \\vert \\ x, y) \\\\\n",
    "                      & = \\arg \\max_\\theta \\frac{p(x ,y,\\theta)}{p(x,y)} \\\\\n",
    "                      & = \\arg \\max_\\theta \\frac{p(y \\ \\vert \\ x, \\theta) \\ p(\\theta \\ \\vert \\ x) \\ p(x)}{p(x, y)} \\\\\n",
    "                      & = \\arg \\max_\\theta \\frac{p(y \\ \\vert \\ x, \\theta) \\ p(\\theta) \\ p(x)}{p(x, y)} \\\\\n",
    "                      & = \\arg \\max_\\theta p(y \\ \\vert \\ x, \\theta) \\ p(\\theta)\n",
    "\\end{align*}\n",
    "\n",
    "And by assumption, $p(\\theta|x)=p(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234edf5-14ab-45cc-a605-d46382cafab1",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "Since $p(\\theta) \\sim \\mathcal{N} (0, \\eta^2 I)$,\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\mathrm{MAP}} & = \\arg \\max_\\theta p(y \\ \\vert \\ x, \\theta) \\ p(\\theta) \\\\\n",
    "                      & = \\arg \\min_\\theta - \\log p(y \\ \\vert \\ x, \\theta) - \\log p(\\theta) \\\\\n",
    "                      & = \\arg \\min_\\theta - \\log p(y \\ \\vert \\ x, \\theta) - \\log \\frac{1}{(2 \\pi)^{d / 2} \\vert \\Sigma \\vert^{1/2}} \\exp \\big( -\\frac{1}{2} (\\theta - \\mu)^T \\Sigma^{-1} (\\theta - \\mu) \\big) \\\\\n",
    "                      & = \\arg \\min_\\theta - \\log p(y \\ \\vert \\ x, \\theta) + \\frac{1}{2} \\theta^T \\Sigma^{-1} \\theta \\\\\n",
    "                      & = \\arg \\min_\\theta - \\log p(y \\ \\vert \\ x, \\theta) + \\frac{1}{2 \\eta^2} \\Vert \\theta \\Vert_2^2 \\\\\n",
    "                      & = \\arg \\min_\\theta - \\log p(y \\ \\vert \\ x, \\theta) + \\lambda \\Vert \\theta \\Vert_2^2\n",
    "\\end{align*}\n",
    "\n",
    "where $\\lambda = 1 / (2 \\eta^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216113fa-310f-46ba-8b39-b25307c64911",
   "metadata": {},
   "source": [
    "**(c)**\n",
    "\n",
    "Our model for the whole training set can be written effectively as $\\vec{y}=X\\theta+\\vec{\\epsilon}$ where $\\vec{\\epsilon} \\sim \\mathcal{N}(0,\\sigma^2I)$. Then, $\\vec{y}|X,\\theta \\sim \\mathcal{N}(X\\theta, \\sigma^2I)$. Using the result from (b), we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\mathrm{MLE}} & = \\arg \\max_\\theta \\prod_{i=1}^m p(y^{(i)}|x^{(i)},\\theta) \\\\\n",
    "                      & = \\arg \\max_\\theta \\prod_{i=1}^m \\frac{1}{\\sqrt{2\\pi}\\theta}\\text{exp}\\{-\\frac{1}{2\\sigma^2}(y^{(i)}-\\theta^T x^{(i)})^2\\} \\\\\n",
    "                      & \\arg \\max_\\theta \\frac{1}{(2\\pi)^{m/2}\\sigma^m}\\text{exp}\\{-\\frac{1}{2\\sigma^2}(y^{(i)}-\\theta^T x^{(i)})^2\\} \\\\\n",
    "                      & \\arg \\max_\\theta \\frac{1}{(2\\pi)^{m/2}\\sigma^m}\\text{exp}\\{-\\frac{1}{2\\sigma^2}(\\parallel X\\theta-\\vec y \\parallel_2^2)\n",
    "\\end{align*}\n",
    "\n",
    "$$\\text{log}p(\\vec{y}|X,\\theta)=-\\frac{m}{2}\\text{log}(2\\pi)-m\\text{log} \\sigma -\\frac{1}{2\\sigma^2}\\parallel X\\theta-\\vec y \\parallel_2^2$$\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\mathrm{MAP}} & = \\arg \\min_\\theta - \\text{log}p(y|x,\\theta)+ \\lambda \\Vert \\theta \\Vert_2^2 \\\\\n",
    "                      & = \\arg \\min_\\theta \\frac{1}{2 \\sigma^2} (\\vec{y} - X \\theta)^T (\\vec{y} - X \\theta) + \\frac{1}{2 \\eta^2} \\Vert \\theta \\Vert_2^2 \\\\\n",
    "                      & = \\arg \\min_\\theta J(\\theta)\n",
    "\\end{align*}\n",
    "\n",
    "By solving\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J(\\theta) & = \\nabla_\\theta \\big( \\frac{1}{2 \\sigma^2} (\\vec{y} - X \\theta)^T (\\vec{y} - X \\theta) + \\frac{1}{2 \\eta^2} \\Vert \\theta \\Vert_2^2 \\big) \\\\\n",
    "                        & = \\frac{1}{2 \\sigma^2} \\nabla_\\theta (\\theta^T X^T X \\theta - 2 \\vec{y}^T X \\theta + \\frac{\\sigma^2}{\\eta^2} \\theta^T \\theta) \\\\\n",
    "                        & = \\frac{1}{\\sigma^2} (X^T X \\theta - X^T \\vec{y} + \\frac{\\sigma^2}{\\eta^2} \\theta) \\\\\n",
    "                        & = 0\n",
    "\\end{align*}\n",
    "\n",
    "we obtain\n",
    "\n",
    "$$\\theta_{\\mathrm{MAP}} = (X^T X + \\frac{\\sigma^2}{\\eta^2} I)^{-1} X^T \\vec{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497db87f-cdca-46d9-8d18-d8f239aa9eee",
   "metadata": {},
   "source": [
    "**(d)**\n",
    "\n",
    "Assume $\\theta \\in \\mathbb{R}^n$. Given $\\theta_i \\sim \\mathcal{L} (0, bI)$ and $y = \\theta^T x + \\epsilon$ where $\\epsilon \\sim \\mathcal{N} (0, \\sigma^2)$, we have\n",
    "\n",
    "$$p(\\theta)=\\frac{1}{(2b)^n}\\text{exp}\\{-\\frac{1}{b} \\Vert \\theta \\Vert \\}$$\n",
    "\n",
    "$$\\text{log}p(\\theta)=-n\\text{log}(2b)-\\frac{1}{b}\\Vert \\theta \\Vert_1$$\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\mathrm{MAP}} & = \\arg \\min_\\theta \\frac{1}{2 \\sigma^2} \\Vert X\\theta -\\vec{y} \\Vert_2^2 -\\text{log}p(\\theta) \\\\\n",
    "                      & = \\arg \\min_\\theta \\frac{1}{2 \\sigma^2} \\Vert X \\theta - \\vec{y} \\Vert_2^2 + \\frac{1}{b} \\Vert \\theta \\Vert_1 \\\\\n",
    "                      & = \\arg \\min_\\theta \\Vert X \\theta - \\vec{y} \\Vert_2^2 + \\frac{2 \\sigma^2}{b} \\Vert \\theta \\Vert_1\n",
    "\\end{align*}\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$J(\\theta)=\\Vert X\\theta-\\vec y \\Vert_2^2 +\\gamma \\Vert \\theta \\Vert_1$$\n",
    "\n",
    "$$\\theta_{MAP}=\\arg \\min_\\theta J(\\theta)$$\n",
    "\n",
    "$$\\gamma=\\frac{2\\sigma^2}{b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c37b01b-42b4-4573-a549-8ba9a624da25",
   "metadata": {},
   "source": [
    "**Remark:** Linear regression with $L_2$ regularization is also commonly called *Ridge regression*, and when $L_1$ regularization is employed, is commonly called *Lasso regression*. These regularization can be applied to any Generalized Linear models just as above (by replacing $\\text{log}p(y|x,\\theta)$ with the appropriate family likelihood). Regularization techniques of the above type are also called *weight decay*, and *shrinkage*. The Gaussian and Laplace priors encourage the parameter values to be closer to their mean ($i.e$ zero), which results in the shrinkage effect.\n",
    "\n",
    "**Remark:** Lasso regression ($i.e.L_1$ regularization) is known to result in sparse parameters, where most of the parameter values are zero, with only some of them non-zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
