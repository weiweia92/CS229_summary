{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6376c9-7606-4c7c-9bab-c1ce528cad9e",
   "metadata": {},
   "source": [
    "### Neural Networks: MNIST image classification\n",
    "\n",
    "**(a)**\n",
    "\n",
    "For an input example $x^{(i)}$ of class $l$, the loss function:\n",
    "\n",
    "$$CE(y^{(i)},\\hat{y}^{(i)})=-\\sum_{k=1}^K y_k^{(i)}\\text{log}y_k^{(i)}=-\\text{log}\\hat{y_l}^{(i)}=-\\text{log}\\Big(\\frac{\\text{exp}(z_l^{(i)})}{\\sum_{k=1}^K\\text{exp}(z_k^{(i)})}\\Big)=\\text{log}\\Big(\\sum_{k=1}^K \\text{exp}(z_k^{(i)})\\Big)-z_l^{(i)}$$\n",
    "\n",
    "Consider each component $j$ of the gradient, when $j=l$ we have:\n",
    "\n",
    "$$\\frac{\\partial CE(y^{(i)},\\hat{y}^{(i)})}{\\partial z_l^{(i)}}=\\frac{\\text{exp}(z_l^{(i)})}{\\sum_{k=1}^K\\text{exp}(z_k^{(i)})}-1=\\hat{y_l}^{(i)}-y_l^{(i)}$$\n",
    "\n",
    "When $j \\neq l$, then\n",
    "\n",
    "$$\\frac{\\partial CE(y^{(i)},\\hat{y}^{(i)})}{\\partial z_j^{(i)}}=\\frac{\\text{exp}(z_j^{(i)})}{\\sum_{k=1}^K\\text{exp}(z_k^{(i)})}=\\hat{y_j}^{(i)}-y_j^{(i)}$$ \n",
    "\n",
    "as $y_j^{(i)}=0$\n",
    "\n",
    "These imply that $\\Delta_{z^{(i)}}CE(y^{(i)},\\hat{y}^{(i)})=\\hat{y}^{(i)}-y^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f65ba-7f75-4a60-9fe7-ef0b55451c10",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "Let's change some notations to make our life easier. The forward propagation computation for an example $(x,y)$ is re-written equivalently as follows:\n",
    "\n",
    "$$z^{[1]}=W^{[1]}+b^{[1]}$$\n",
    "\n",
    "$$a^{[1]}=\\sigma(z^{[1]})$$\n",
    "\n",
    "$$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$$\n",
    "\n",
    "$$\\hat{y}=\\text{softmax}(z^{[2]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f24bc7-f98c-49d4-9d24-c69a13721490",
   "metadata": {},
   "source": [
    "Note that our weight matrices are transposes of the weights introduced in this sub-problem statement. Let $\\mathcal{L}$ be the cross entropy loss of a single training example $(x, y)$. Using matrix calculus, we obtain the following formulas:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}}=\\hat{y}-y$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}}=\\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}}\\cdot \\frac{\\partial z^{[2]}}{\\partial W^{[2]}}=\\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}}{a^{[1]}}^T,\\frac{\\partial \\mathcal{L}}{\\partial b^{[2]}}=\\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial a^{[1]}}={W^{[2]}}^T\\cdot \\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}},\\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}}=\\sigma'(z^{[1]})\\odot \\frac{\\partial \\mathcal{L}}{\\partial a^{[1]}}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W^{[1]}}=\\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}}x^T,\\frac{\\partial \\mathcal{L}}{\\partial b^{[1]}}=\\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70113ad4-1721-400b-a547-aa35050d7a40",
   "metadata": {},
   "source": [
    "**(c)**\n",
    "\n",
    "These model attained the same level of accuracy on the training set which was actually almost optimal. But the gap from the training to the dev accuracy is greater in the non-regularized baseline model. As a heuristic, this characteristic might indicate that compared to the regularized model, the baseline endured larger variance problem. Regularization did help in this case. And we expect better test accuracy on the second model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0faed46-76dd-490d-aa76-fbbbb0d9c9e6",
   "metadata": {},
   "source": [
    "**(d)**\n",
    "\n",
    "Our model had the test accuracy 0.932 without regularization and 0.9653 with regularization. Regularized models often offer better generalization (apparently after some tuning), which leads to better test accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
