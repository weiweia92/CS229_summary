{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8623de98-51b8-4d8a-907c-dc61d8a564e1",
   "metadata": {},
   "source": [
    "## Introduction to Independent Component Analysis in Machine Learning\n",
    "\n",
    "### What is Independent Component Analysis?\n",
    "\n",
    "Independent Component Analysis (ICA) is a technique in statistics used to detect(探测) hidden factors that exist in datasets of random variables, signals, or measurements. \n",
    "\n",
    "An alternative to principal component analysis (PCA), Independent Component Analysis helps to divide multivariate signals into subcomponents that are assumed to be **non-Gaussian in nature, and independent of each other**. \n",
    "\n",
    "In signal processing, a field of electrical engineering, ICA plays an important role in helping segregate(隔离) independent components of electrical signals like sound, image, etc. \n",
    "\n",
    "For instance, in the case of separating sound sources, ICA transforms different vectors into independent datasets that help to determine the sources of sound signals. Invented in 1986, the concept of ICA was formulated by Herault and Jutten in an attempt to solve the problem of Blind Source Separation (BSS) in signal processing. Although this concept resembles the PCA system, it is an aggressive(积极的) way to detect independent subcomponents in a dataset, and in a much more accurate way. That said, the ICA is known for its efficiency in the field of Machine Learning and statistics. \n",
    "\n",
    "### How does ICA work?\n",
    "\n",
    "A typical independent component analysis example is the cocktail party problem. Herein, the cocktail party problem stands for its literal scenario that creates a noisy environment where people talking to each other cannot hear what the other person has said. \n",
    "\n",
    "What’s more, the noisy environment also leads to a mixing of sound signals that, in turn, disables people from identifying the source of sound signals. This is where Independent Component Analysis steps in. \n",
    "\n",
    "For this experiment, we take 2 mixed audio recordings. To unmix and segregate the two audio recordings, the ML algorithm detects the different sets of vectors by measuring the sound signals. \n",
    "\n",
    "The number of inputs registered by the algorithm is equal to the number of outputs produced. Herein, two assumptions are made before applying the technique of ICA. \n",
    "\n",
    "One, that the two signals are ‘statistically independent’ of each other (unaffected by the occurrence of each other), and two, that these subcomponents are non-Gaussian (abnormal(异常) distribution) in nature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca9b7e-3452-4478-8472-12b8b0791236",
   "metadata": {},
   "source": [
    "While the dependence of one component over the other might lead to confusion between the measurements of two different signals, statistical independence reflects that a phenomenon can occur without affecting or hampering the probability of occurrence of the other one. \n",
    "\n",
    "Moreover, non-Gaussian distribution implies that latent(潜在) factors in a linear mixture of a multivariate dataset can be separated from each other, unlike Gaussian distribution that leads to a similarity in statistical values of two variables.\n",
    "\n",
    "Even though the mixing process of signals is unknown to the algorithms, they are still able to detect the source of various signals based on the technique of Independent Component Analysis.\n",
    "\n",
    "### ICA in Machine Learning\n",
    " \n",
    "While Independent Component Analysis is largely known to be a statistical technique that helps to detangle(解开) a dataset to identify hidden factors that are independent, ICA can also be seen as one of the most sought-after(追捧的) Machine Learning techniques that is used to train algorithms for various applications. \n",
    "\n",
    "In a series of neural networks and random datasets that originate under them, finding independent factors and segregating them from the other Gaussian data points is a substantial task. But, with the help of ICA-empowered signal processing algorithms, one can very well perform the task of analyzing independent components hidden in a dataset. \n",
    "\n",
    "ICA in machine learning tools and techniques has advanced significantly and has also enabled mankind to perform a variety of functions like image processing and Biomedical Signal Processing. \n",
    "\n",
    "Unlike Principal Component Analysis in Machine Learning that focuses on the variance of components, whether Gaussian or non-Gaussian, ICA emphasizes the interdependence of components that are latent in between the dependent variables in a dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee6b71d-2194-44b2-98e5-07d70caa2f03",
   "metadata": {},
   "source": [
    "### 6. Independent components analysis\n",
    "\n",
    "**(a) Gaussian source**\n",
    "\n",
    "In ICA, we want to maximize as a function of W the following objective:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(W)&= \\sum_{i=1}^n \\text{log} p_x(x^{(i)}) \\tag{1}\\\\\n",
    "            &= \\sum_{i=1}^n \\text{log}(p_s(Wx^{(i)})\\vert W \\vert) \\tag{2}\\\\\n",
    "            &= \\sum_{i=1}^n \\text{log}\\Big(\\frac{1}{(2\\pi)^{d/2}}\\text{exp}\\left\\{-\\frac{1}{2}(Wx^{(i)})^T (Wx^{(i)})\\right\\} \\vert W \\vert \\Big) \\tag{3}\\\\\n",
    "            &= \\sum_{i=1}^n \\Big(-\\frac{d}{2}\\text{log}(2\\pi)-\\frac{1}{2}{x^{(i)}}^T W^T W x^{(i)}+\\text{log}\\vert W \\vert \\Big)\\tag{4}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b169279-8cae-4869-b31b-5e3e223e810f",
   "metadata": {},
   "source": [
    "line(3): Because we assume sources are distributed according to a standard normal distribution, i.e. $s_j \\sim \\mathcal{N}(0,1),j=\\{1,\\cdots, d\\}$\n",
    "\n",
    "To maximize this objective, we will compute its gradient and set it equal to 0. We have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_W \\ell(W)&= \\sum_{i=1}^n \\big(-\\frac{1}{2} \\nabla_W {x^{(i)}}^T W^T W x^{(i)}+\\nabla_W \\text{log} \\vert W \\vert\\big) \\tag{5}\\\\\n",
    "            &= \\sum_{i=1}^n \\big(-W x^{(i)} {x^{(i)}}^T +(W^{-1})^T \\big)\\tag{6}\\\\\n",
    "            &= -W \\big( \\sum_{i=1}^n x^{(i)} {x^{(i)}}^T \\big)+n(W^{-1})^T \\tag{7}\\\\\n",
    "            &= -W X^T X + n(W^{-1})^T\\tag{8}\n",
    "\\end{align*}\n",
    "\n",
    "Set this equal to 0, we get: $W^T W=(\\frac{1}{n}X^T X)^{-1}$, assuming that the right-hand side is invertible. Let $Y=(\\frac{1}{n}X^T X)^{-1}$, then $Y$ is positive semi-definite. According to linear algebra, we can decompose $W=U\\Sigma V^T$ where $U,V$ are orthogonal and $\\Sigma$ is diagonal. Then, we have:\n",
    "\n",
    "$$W^T W=(V\\Sigma U^T)(U \\Sigma V^T)=V\\Sigma(U^T U)\\Sigma V^T=V\\Sigma^2 V^T=Y$$\n",
    "\n",
    "And so, we can compute the eigendecomposition of $Y$ to recover $\\Sigma^{*}, V^{*}$ and reconstruct $W=U\\Sigma^{*}V^{*}$ with an arbitrary orthogonal matrix $U$ . This arbitrary rotational component(旋转分量) $U$ can not be determined from the data $X$ which leads to ambiguity. The ICA can not recover the original sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e5cd7-6f2c-44ba-9bd2-3704c220f8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
