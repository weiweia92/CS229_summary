{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e864c4-d335-46a8-8a2e-5fd4248331db",
   "metadata": {},
   "source": [
    "**(a) Nonnegativity**\n",
    "\n",
    "Kullback-Leibler Divergence is just a slight modification of our formula for entropy. Rather than just having our probability distribution $p$ we add in our approximating distribution $q$. Then we look at the difference of the log values for each:\n",
    "\n",
    "$$D_{KL}(p\\Vert q)=\\sum_{i=1}^N p(x_i)\\cdot (\\text{log}p(x_i)-\\text{log}q(x_i))$$\n",
    "\n",
    "Essentially, what we're looking at with the KL divergence is the expectation(期望) of the log difference between the probability of data in the original distribution with the approximating distribution. Again, if we think in terms of $\\text{log}_2$ we can interpret this as \"how many bits of information we expect to lose\". We could rewrite our formula in terms of expectation:\n",
    "\n",
    "$$D_{KL}(p\\Vert q)=E[\\text{log}p(x)-\\text{log}q(x)]$$\n",
    "\n",
    "[Hint: You may use the following result, called **Jensen’s inequality**. If f is a convex function, and X is a random variable, then $E[f (X )] \\geq f (E[X ])$ . Moreover, if $f$ is strictly convex ($f$ is convex if its Hessian satisﬁes $H \\geq 0$; it is strictly convex if $H > 0$; for instance $f (x) = −\\text{log}x$ is strictly convex), then $E[f (X )] = f (E[X ])$ implies that $X = E[X ]$ with probability 1; i.e., X is actually a constant.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed208685-4fa7-44a1-a6c3-d7b91954bc4c",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "We have \n",
    "\n",
    "$$D_{KL}(P \\Vert Q)=\\mathbb{E}_{X\\sim P}\\big[-\\text{log}\\frac{Q(X)}{P(X)}\\big]\\geq-\\text{log}\\mathbb{E}_{X\\sim P}\\big[\\frac{Q(X)}{P(X)}\\big]\\tag{1}$$\n",
    "\n",
    "$$=-\\text{log}\\sum_x P(x)\\frac{Q(x)}{P(x)}=-\\text{log}sum_xQ(x)=-\\text{log}(1)=0$$\n",
    "\n",
    "In line (1), we applied the Jensen's inequality for the strictly convex function $-\\text{log}(\\cdot)$. The inequality holds with equality if and  only if $\\frac{Q(X)}{P(X)}$ is a constant almost surely. This implies that $\\frac{Q(X)}{P(X)}$ = a constant for $x\\in$ the support of $X$, which implies $P=Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1cf831-d4c9-4afb-8096-661403225236",
   "metadata": {},
   "source": [
    "**(b) Chain rule for KL divergence**\n",
    "\n",
    "\\begin{align*}\n",
    "D_{KL}(P(X,Y)\\Vert Q(X,Y))& = \\sum_x \\sum_y P(x,y)\\text{log}\\frac{P(x,y)}{Q(x,y)} \\\\\n",
    "        & = \\sum_x \\sum_y P(x,y)\\text{log}\\frac{P(x,y)Q(y|x)}{Q(x,y)P(y|x)} \\\\\n",
    "        & = \\sum_x \\sum_y P(x,y)\\text{log}\\frac{P(x,y)}{Q(x,y)}+\\sum_x \\sum_y P(x,y)\\text{log}\\frac{P(y|x)}{Q(y|x)} \\\\\n",
    "        & = \\sum_x \\sum_y P(x,y)\\text{log}\\frac{P(x,y)}{Q(x,y)}+\\sum_x \\sum_y P(y)P(x|y)\\text{log}\\frac{P(y|x)}{Q(y|x)} \\\\\n",
    "        & = \\sum_x P(x)\\text{log}\\frac{P(x)}{Q(x)}+\\sum_y P(y) \\sum_x P(x|y)\\text{log}\\frac{P(y|x)}{Q(y|x)} \\\\\n",
    "        & = D_{KL}(P(X)\\Vert Q(X))+D_{KL}(P(Y|X)\\Vert Q(Y|X))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ead42c-1cf3-4b65-89dd-da97de637d85",
   "metadata": {},
   "source": [
    "**(c) KL and maximum likelihood**\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{arg}\\underset{\\theta}{\\text{min}}D_{KL}(\\hat P \\Vert P_{\\theta})&= \\text{arg}\\underset{\\theta}{\\text{min}}\\sum_x \\hat P(x) \\text{log} \\frac{\\hat P(x)}{P_{\\theta}(x)} \\tag{1}\\\\\n",
    "            &= \\text{arg}\\underset{\\theta}{\\text{min}}- \\sum_x \\hat P(x) \\text{log} \\frac{P_{\\theta}(x)}{\\hat P(x)} \\tag{2}\\\\\n",
    "            &= \\text{arg}\\underset{\\theta}{\\text{min}}-\\sum_x\\frac{1}{m}\\sum_{i=1}^{m} 1\\{x^{(i)} = x\\} \\text{log}\\frac{P_{\\theta}(x)}{\\frac{1}{m} \\sum_{i=1}^{m} 1 \\{x^{(i)} = x\\}} \\tag{3}\\\\\n",
    "            &= \\text{arg}\\underset{\\theta}{\\text{max}}\\frac{1}{m}\\sum_{i=1}^{m}\\sum_x 1\\{x^{(i)} = x\\} \\text{log}\\frac{P_{\\theta}(x)}{\\frac{1}{m} \\sum_{i=1}^{m} 1 \\{x^{(i)} = x\\}}\\tag{4}\\\\\n",
    "&= \\text{arg}\\underset{\\theta}{\\text{max}}\\sum_{i=1}^{m}\\sum_x 1\\{x^{(i)} = x\\} \\text{log}P_{\\theta}(x)\\tag{5} \\\\\n",
    "&= \\text{arg}\\underset{\\theta}{\\text{max}}\\sum_{i=1}^{m}\\text{log}P_{\\theta}(x^{(i)})\\tag{6} \n",
    "\\end{align*}\n",
    "\n",
    "In line (5), the denominator doesn't depend on $\\theta$ and can be safely removed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
